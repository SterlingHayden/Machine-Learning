{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent, Regression, MLE and MSE\n",
    "Sterling Hayden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note: I had ChatGPT fromat all my equations into LaTeX format._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter update rule for Simple Linear Regression using Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear Regression Equation:**\n",
    "\n",
    "The linear regression equation is defined as:\n",
    "\n",
    "$$y = b_0 + b_1x_1 + b_2x_2 + \\ldots + b_Jx_J$$\n",
    "\n",
    "Where:\n",
    "- \\(y\\) is the predicted output (dependent variable).\n",
    "- \\(x_1, x_2, \\..., x_J\\) are the Jth input features (independent variables).\n",
    "- \\(b_0\\) is the y-intercept.\n",
    "- \\(b_1, b_2, \\ldots, b_J\\) are the weights associated with each feature.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1:** Define Mean Squared Error (MSE):\n",
    "\n",
    "$$MSE = \\frac{1}{2N} \\sum_{i=1}^{N} \\left(y_i - \\left(b_0 + \\sum_{j=1}^{J} b_j x_{ij}\\right)^2\\right)$$\n",
    "\n",
    "Where:\n",
    "- \\(N\\) is the number of data points.\n",
    "- \\(i\\) indexes the data points.\n",
    "- \\(j\\) indexes the features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2:** Partial derivatives of MSE  \n",
    "   \n",
    "For b<sub>0</sub>:  \n",
    "$$\n",
    "\\frac{\\partial MSE}{\\partial b_0} = -\\frac{1}{N} \\sum \\left(y_i - \\left(b_0 + \\sum b_j x_{ij}\\right)\\right)\n",
    "$$\n",
    "  \n",
    "For b<sub>j</sub>:  \n",
    "$$\n",
    "\\frac{\\partial MSE}{\\partial b_j} = -\\frac{1}{N} \\sum x_{ij} \\left(y_i - \\left(b_0 + \\sum b_j x_{ij}\\right)\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3:** Update weights  \n",
    "$$b_0 \\leftarrow b_0 - \\frac{\\partial MSE}{\\partial b_0}$$\n",
    "\n",
    "$$b_j \\leftarrow b_j - \\frac{\\partial MSE}{\\partial b_j}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4:** Repeat steps 2 and 3 until convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several libraries in Python that can perform linear regression, including gradient descent, and make it more convenient for us. One of the most commonly used libraries is scikit-learn. Here's how we can use scikit-learn to perform linear regression easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept (b0): 3.9502139024683567\n",
      "Slope (b1): 2.7956623869503465\n",
      "Slope (b2): 1.9899783163480775\n",
      "Slope (b3): -1.3733845071909832\n"
     ]
    }
   ],
   "source": [
    "# imports for LR\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "#  gen data\n",
    "np.random.seed(0)\n",
    "X = 2 * np.random.rand(100, 3)\n",
    "y = 4 + 3*X[:, 0] + 2*X[:, 1] - 1.5*X[:, 2] + np.random.randn(100)\n",
    "\n",
    "# Create LR model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Fit model\n",
    "model.fit(X, y)\n",
    "\n",
    "# Get weights\n",
    "intercept = model.intercept_\n",
    "coefficients = model.coef_\n",
    "\n",
    "print(\"Intercept (b0):\", intercept)\n",
    "print(\"Slope (b1):\", coefficients[0])\n",
    "print(\"Slope (b2):\", coefficients[1])\n",
    "print(\"Slope (b3):\", coefficients[2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter update rule for Logistic Regression using Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Logistic Regression, the sigmoid function (also known as the logistic function) is used to model the probability of a binary outcome. The sigmoid function is defined as:  \n",
    "\n",
    "$$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "\n",
    "where z is the linear combination of the feature values and the model parameters:  \n",
    "\n",
    "$$z = \\mathbf{w} \\cdot \\mathbf{x}$$\n",
    "\n",
    "\n",
    "Here, W represents the parameter vector and x is the feature vector for a given data point.  \n",
    "The probability of the positive class (\\(y = 1\\)) can be expressed as:  \n",
    "\n",
    "$$p(y=1|\\mathbf{x}) = \\sigma(\\mathbf{w} \\cdot \\mathbf{x})$$\n",
    "\n",
    "\n",
    "And the probability of the negative class (\\(y = 0\\)) is the complement:  \n",
    "\n",
    "$$p(y=0|\\mathbf{x}) = 1 - p(y=1|\\mathbf{x}) = 1 - \\sigma(\\mathbf{w} \\cdot \\mathbf{x})$$\n",
    "\n",
    "\n",
    "To derive the parameter update rule for Logistic Regression using Gradient Descent in terms of the sigmoid function, we need to calculate the gradient of the log-likelihood. Here's the gradient with respect to the parameter w_j:  \n",
    "\n",
    "$$\\frac{\\partial \\log L(\\mathbf{w})}{\\partial w_j} = \\sum_{i=1}^{N} \\left( y_i - \\sigma(\\mathbf{w} \\cdot \\mathbf{x}_i) \\right) \\cdot x_{ij}$$\n",
    "\n",
    "\n",
    "The parameter update rule for Logistic Regression using Gradient Descent in terms of the sigmoid function is then:  \n",
    "\n",
    "$$w_j \\leftarrow w_j + \\alpha \\cdot \\sum_{i=1}^{N} \\left( y_i - \\sigma(\\mathbf{w} \\cdot \\mathbf{x}_i) \\right) \\cdot x_{ij}$$\n",
    "\n",
    "\n",
    "This update rule is applied to each parameter w_j iteratively to learn the optimal values that maximize the likelihood of the observed data under the logistic regression model. The learning rate alpha controls the step size in the gradient descent process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is the code interpretation of logistic regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept (b0): [0.40279443]\n",
      "Coefficient (b1): -0.0011113147498026527\n",
      "Coefficient (b2): -0.2721459386001874\n",
      "Coefficient (b3): 0.06241684222277979\n"
     ]
    }
   ],
   "source": [
    "# imports for LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "\n",
    "# data gen\n",
    "np.random.seed(0)\n",
    "X = 2 * np.random.rand(100, 3)\n",
    "y = (np.random.rand(100) < 0.5).astype(int)\n",
    "\n",
    "# init model\n",
    "model = LogisticRegression()\n",
    "\n",
    "model.fit(X, y)\n",
    "intercept = model.intercept_\n",
    "coefficients = model.coef_\n",
    "\n",
    "print(\"Intercept (b0):\", intercept)\n",
    "print(\"Coefficient (b1):\", coefficients[0][0])\n",
    "print(\"Coefficient (b2):\", coefficients[0][1])\n",
    "print(\"Coefficient (b3):\", coefficients[0][2])\n",
    "# Positive coefficients increase the likelihood of the positive class, while negative coefficients decrease it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proof that MSE is a special case of MLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Linear Regression we assume our model is gaussian. In other words we assume y follows a normal distribution.  \n",
    "  \n",
    "Thus the Likelihood of y:  \n",
    "$$\n",
    "L(u,\\sigma|X) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\prod_i^n \\exp\\left(-\\frac{(y - u)^2}{2\\sigma^2}\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\rightarrow    \\alpha(B, \\sigma^2) = (\\frac{1}{\\sqrt{2\\pi\\sigma^2}})^n \\prod_i^n \\exp\\left(-\\frac{(y_i - B^T x_i)^2}{2\\sigma^2}\\right)\n",
    "$$\n",
    "\n",
    "Therefore, log-likelihood is:  \n",
    "$$\n",
    "\\log L(B, \\sigma^2) = \\log\\left[\\frac{1}{\\sqrt{2\\pi\\sigma^2}})^n \\prod_i^n \\exp\\left(-\\frac{(y_i - B^T x_i)^2}{2\\sigma^2}\\right)\\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\rightarrow \\log L(B, \\sigma^2) = n\\log\\left(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\right) + \\sum_{i}^n \\frac{-(y_i - B^T x_i)^2}{2\\sigma^2}\n",
    "$$\n",
    "\n",
    "If we say the negitive of likelihood is our loss:  \n",
    "$$\n",
    "loss = -\\log L(B,\\sigma^2)\n",
    "$$\n",
    "\n",
    "Recall that we are intrested in the optimal solution rather than the optimal value.  \n",
    "In other words we want to minimize the negitive likelihood, which we can find by setting the partial deriv to 0.    \n",
    "$$\n",
    "\\hat{B} = argmin_B L(B, \\sigma)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\rightarrow\\hat{B} = argmin_B \\sum_{i}^n (y_i - B^T x_i)^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\rightarrow\\hat{B} = argmin_B \\sum_{i}^n (y_i - \\hat{y}_o)^2\n",
    "$$ \n",
    "Which is the sum of sqaure error.  \n",
    "\n",
    "Hence in Linear regression, MSE is a special case of MLE when we assume the model is Gaussian.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
