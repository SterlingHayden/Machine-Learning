{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SterlingHayden/Machine-Learning/blob/main/SVM/Functions_and_Libraries.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3: List of Functions and Libraries\n"
      ],
      "metadata": {
        "id": "6GaG6PMPM6Xs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pojztgBHMvLZ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd # for data manipulation\n",
        "import matplotlib.pyplot as plt # for plotting\n",
        "import seaborn as sns # for statistical ploting\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor # used to calculate VIF in order to drop columns with multicollinearity\n",
        "from sklearn.model_selection import train_test_split  # train test split made easy\n",
        "from sklearn.svm import SVC # SVC = Support vector Classifier, finds hyperplane that best splits classes\n",
        "from sklearn.preprocessing import StandardScaler # used for feature scaling, needed for standerdizing in our case\n",
        "from sklearn.metrics import accuracy_score  # calculating model accuracy made easy\n",
        "from sklearn.metrics import confusion_matrix # for creating confusion matrix\n",
        "from itertools import product # creating all pairwise combinations from two lists\n",
        "from sklearn.decomposition import PCA # principal component analysis for data transformation\n",
        "from sklearn.linear_model import LogisticRegression # logistic regression for classification\n",
        "from sklearn.decomposition import KernelPCA # kernel principal component analysis for non-linear data transformation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   `import pandas as pd` facilitates data manipulation using the Pandas library.\n",
        "*   `import matplotlib.pyplot as plt` enables data visualization through Matplotlib, a popular plotting library.\n",
        "*  ` import seaborn as sns` provides capabilities for statistical plotting, enhancing the visual representation of data.\n",
        "*   `from statsmodels.stats.outliers_influence import variance_inflation_factor` allows you to calculate the Variance Inflation Factor (VIF) for assessing multicollinearity. `variance_inflation_factor(X, i)` takes in function takes a design matrix (X) and the index of the variable (i) and outputs the Variance Inflation Factor.\n",
        "*   `from sklearn.model_selection import train_test_split` splits a dataset into training and testing subsets. `train_test_split(X, y, test_size, random_state)` takes in 'X' the feature data, 'y' the response data, 'test_size' what % of our data we want to set aside for testing, and 'random_state' an optinal param used to ensure reproducibility. The outcome comprises four distinct subsets: 'X_train,' the feature dataset for training, 'X_test,' the feature dataset for testing, 'y_train,' representing the target variable for training, and 'y_test,' signifying the target variable for testing.\n",
        "*   `from sklearn.svm import SVC` provides a support vector classifier for finding a hyperplane that best splits classes in classification tasks. You provide parameters like kernel type, regularization parameter C, and training data. After training, it returns a trained classifier that can predict class labels for new data points and can also provide information about the support vectors.\n",
        "*   `from sklearn.preprocessing import StandardScaler` is to standardize or normalize features in a dataset, ensuring that they have zero mean and unit variance, which is essential for support vector machines. The `StandardScaler` allows for customization through input parameters like 'copy', 'with_mean', and 'with_std'. When applied to the data, it transforms features to have zero mean and unit variance when 'with_mean' and 'with_std' are set to 'True'. The class also provides outputs: a transformed data array and arrays 'mean_' and 'scale_', which contain mean values and standard deviations for each feature in the training data. These outputs ensure consistent scaling of new data, making it an essential tool for machine learning models that rely on distance-based metrics and preventing any single feature from dominating the learning process.\n",
        "*   `from sklearn.metrics import accuracy_score` calculates the accuracy of a classification model by comparing the true target values (ground truth) with the predicted values. `accuracy_score(y_true, y_pred)` where 'y_true' is the true value of Y and 'y_pred' is the models prediction for Y. Results in float value representing the accuracy score.\n",
        "*   `from sklearn.metrics import confusion_matrix` is a tool to evaluating the performance of a classification model on a deeper level compaired to accuracy.`confusion_matrix(y_true, y_pred)` where 'y_true' is the true value of Y and 'y_pred' is the models prediction for Y. Results in a 2D array that quantifies the number of true positives, true negatives, false positives, and false negatives.\n",
        "*   `from itertools import product` is useful for creating all pairwise combinations from two lists. It accepts two lists as arguments and outputs an iterator over all possible pairwise combinations of each element from the inputs lists.\n",
        "*   `from sklearn.decomposition import PCA` provides a principal component analysis method for data transformation. The desired number of principal components and data are the primary inputs. It outputs the transformed data in a new space, where the principal components capture the most significant variance in the data.\n",
        "*   `from sklearn.linear_model import LogisticRegression` provides a logistic regression classifier. Training data is inputted into the model. After training, the trained model can predict class labels for new data points.\n",
        "*   `from sklearn.decomposition import KernelPCA` provides a kernel-based principal component analysis method for non-linear data transformation. You pass the kernel function, gamma or degree parameters, and other hyperparameters. The output is similar to PCA, but it transforms data into a higher-dimensional space using kernel tricks to capture non-linear relationships."
      ],
      "metadata": {
        "id": "Q5Yy0T3iPiHO"
      }
    }
  ]
}